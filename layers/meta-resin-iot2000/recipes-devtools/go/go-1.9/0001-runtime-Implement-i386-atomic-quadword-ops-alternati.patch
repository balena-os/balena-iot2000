From 63085d69a1315100a8a1e0500052747fb8d0cec7 Mon Sep 17 00:00:00 2001
From: Jan Kiszka <jan.kiszka@siemens.com>
Date: Mon, 17 Jul 2017 23:31:13 +0200
Subject: [PATCH 1/3] runtime: Implement i386 atomic quadword ops alternative
 with cmpxchg8b

Inspired by patch originally written by Petros Angelatos:

Detect if we are running on a MMX-capable CPU. If not, fall back to
cmpxchg8b-based 64-bit atomic ops. This allows to run these operations
on 486 and Pentium CPUs which lack MMX support, like the Intel Quark.

Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>

Signed-off-by: Florin Sarbu <florin@resin.io>
---
 src/runtime/internal/atomic/asm_386.s | 32 +++++++++++++++++++++++++++-----
 src/sync/atomic/asm_386.s             | 32 +++++++++++++++++++++++++++-----
 2 files changed, 54 insertions(+), 10 deletions(-)

diff --git a/src/runtime/internal/atomic/asm_386.s b/src/runtime/internal/atomic/asm_386.s
index 882906e9ed..7d1db28f6a 100644
--- a/src/runtime/internal/atomic/asm_386.s
+++ b/src/runtime/internal/atomic/asm_386.s
@@ -123,6 +123,8 @@ TEXT runtime∕internal∕atomic·Load64(SB), NOSPLIT, $0-12
 	TESTL	$7, AX
 	JZ	2(PC)
 	MOVL	0, AX // crash with nil ptr deref
+	TESTL	$(1<<23), runtime·cpuid_edx(SB) // check for mmx
+	JEQ	load64_nommx
 	LEAL	ret_lo+4(FP), BX
 	// MOVQ (%EAX), %MM0
 	BYTE $0x0f; BYTE $0x6f; BYTE $0x00
@@ -132,17 +134,28 @@ TEXT runtime∕internal∕atomic·Load64(SB), NOSPLIT, $0-12
 	BYTE $0x0F; BYTE $0x77
 	RET
 
+load64_nommx:
+	MOVL	DX, CX
+	MOVL	AX, BX
+	LOCK
+	CMPXCHG8B (AX)
+	MOVL	AX, ret_lo+4(FP)
+	MOVL	DX, ret_hi+8(FP)
+	RET
+
 // void runtime∕internal∕atomic·Store64(uint64 volatile* addr, uint64 v);
 TEXT runtime∕internal∕atomic·Store64(SB), NOSPLIT, $0-12
-	MOVL	ptr+0(FP), AX
-	TESTL	$7, AX
+	MOVL	ptr+0(FP), SI
+	TESTL	$7, SI
 	JZ	2(PC)
-	MOVL	0, AX // crash with nil ptr deref
+	MOVL	0, SI // crash with nil ptr deref
+	TESTL	$(1<<23), runtime·cpuid_edx(SB) // check for mmx
+	JEQ	store64_nommx
 	// MOVQ and EMMS were introduced on the Pentium MMX.
 	// MOVQ 0x8(%ESP), %MM0
 	BYTE $0x0f; BYTE $0x6f; BYTE $0x44; BYTE $0x24; BYTE $0x08
-	// MOVQ %MM0, (%EAX)
-	BYTE $0x0f; BYTE $0x7f; BYTE $0x00 
+	// MOVQ %MM0, (%ESI)
+	BYTE $0x0f; BYTE $0x7f; BYTE $0x06
 	// EMMS
 	BYTE $0x0F; BYTE $0x77
 	// This is essentially a no-op, but it provides required memory fencing.
@@ -152,6 +165,15 @@ TEXT runtime∕internal∕atomic·Store64(SB), NOSPLIT, $0-12
 	XADDL	AX, (SP)
 	RET
 
+store64_nommx:
+	MOVL	val_lo+4(FP), BX
+	MOVL	val_hi+8(FP), CX
+store64_loop:
+	LOCK
+	CMPXCHG8B (SI)
+	JNE	store64_loop
+	RET
+
 // void	runtime∕internal∕atomic·Or8(byte volatile*, byte);
 TEXT runtime∕internal∕atomic·Or8(SB), NOSPLIT, $0-5
 	MOVL	ptr+0(FP), AX
diff --git a/src/sync/atomic/asm_386.s b/src/sync/atomic/asm_386.s
index f2a13dab66..c4beebd904 100644
--- a/src/sync/atomic/asm_386.s
+++ b/src/sync/atomic/asm_386.s
@@ -157,6 +157,8 @@ TEXT ·LoadUint64(SB),NOSPLIT,$0-12
 	TESTL	$7, AX
 	JZ	2(PC)
 	MOVL	0, AX // crash with nil ptr deref
+	TESTL	$(1<<23), runtime·cpuid_edx(SB) // check for mmx
+	JEQ	load64_nommx
 	// MOVQ and EMMS were introduced on the Pentium MMX.
 	// MOVQ (%EAX), %MM0
 	BYTE $0x0f; BYTE $0x6f; BYTE $0x00
@@ -165,6 +167,15 @@ TEXT ·LoadUint64(SB),NOSPLIT,$0-12
 	EMMS
 	RET
 
+load64_nommx:
+	MOVL	DX, CX
+	MOVL	AX, BX
+	LOCK
+	CMPXCHG8B (AX)
+	MOVL	AX, ret_lo+4(FP)
+	MOVL	DX, ret_hi+8(FP)
+	RET
+
 TEXT ·LoadUintptr(SB),NOSPLIT,$0-8
 	JMP	·LoadUint32(SB)
 
@@ -184,15 +195,17 @@ TEXT ·StoreInt64(SB),NOSPLIT,$0-12
 	JMP	·StoreUint64(SB)
 
 TEXT ·StoreUint64(SB),NOSPLIT,$0-12
-	MOVL	addr+0(FP), AX
-	TESTL	$7, AX
+	MOVL	addr+0(FP), SI
+	TESTL	$7, SI
 	JZ	2(PC)
-	MOVL	0, AX // crash with nil ptr deref
+	MOVL	0, SI // crash with nil ptr deref
+	TESTL	$(1<<23), runtime·cpuid_edx(SB) // check for mmx
+	JEQ	store64_nommx
 	// MOVQ and EMMS were introduced on the Pentium MMX.
 	// MOVQ 0x8(%ESP), %MM0
 	BYTE $0x0f; BYTE $0x6f; BYTE $0x44; BYTE $0x24; BYTE $0x08
-	// MOVQ %MM0, (%EAX)
-	BYTE $0x0f; BYTE $0x7f; BYTE $0x00 
+	// MOVQ %MM0, (%ESI)
+	BYTE $0x0f; BYTE $0x7f; BYTE $0x06
 	EMMS
 	// This is essentially a no-op, but it provides required memory fencing.
 	// It can be replaced with MFENCE, but MFENCE was introduced only on the Pentium4 (SSE2).
@@ -201,5 +214,14 @@ TEXT ·StoreUint64(SB),NOSPLIT,$0-12
 	XADDL	AX, (SP)
 	RET
 
+store64_nommx:
+	MOVL	val_lo+4(FP), BX
+	MOVL	val_hi+8(FP), CX
+store64_loop:
+	LOCK
+	CMPXCHG8B (SI)
+	JNE	store64_loop
+	RET
+
 TEXT ·StoreUintptr(SB),NOSPLIT,$0-8
 	JMP	·StoreUint32(SB)
-- 
2.12.3

